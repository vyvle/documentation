
[id="os-migrate-vmware-guide_vmware"]


= VMware to OpenStack Guide

An important function included in OS Migrate is our VMware tooling,
which allows you to migrate a virtual machine from an ESXi/Vcenter environment
to OpenStack environments.

The code used os-migrate Ansible collection in order to deploy conversion host and setup
correctly the prerequists in the Openstack destination cloud.
It also used the vmware community collection in order to gather informations from the source
VMWare environment.

The Ansible collection provides different steps to scale your migration from VMWare to Openstack:

* A discovery phase where it analizes the VMWare source environment and provides collected data
to help for the migration.
* A pre-migration phase where it make sure the destionation cloud is ready to perform the migration,
by creating the conversion host for example or the required network if needed.
* A migration phase with different workflow where the user can basicaly scale the migration with
a very number of virtual machines as entry point, or can migrate sensitive virtual machine by using
a near zero down time with the change block tracking VMWare option (CBT) and so perform the virtual
machine migration in two steps. The migration can also be done without conversion host.

== Pre-requisites and Validation Checks

Before migrating VMware workloads to OpenStack, ensure the following pre-requisites are met.
These checks are based on issues encountered in real-world migration engagements.

=== Network settings and checks

Before attempting a migration you need to make sure to have the correct network settings and host resolutions.

==== Network checks

Make sure the Vcenter fqdn and OpenStack endpoints can be resolved from the conversion host.
Refer to this section to perform the checks if needed: <<Connectivity from Conversion Host to VMware>>.

==== Network requirements

Ensure that you met all the following network requirements:

|===
| Port / Protocol | Direction | Source / Destination | Purpose

| 443/TCP | Egress | VMware vCenter | Main VMware communication used for authentication, VM metadata, snapshots, and VDDK operations.

| 902/TCP | Egress | VMware ESXi hosts | Direct disk access used to read VM disk data via NFC/NBD protocols.

| 22/TCP | Ingress | Ansible Controller / Admin | Remote management of the conversion host over SSH.

| 10809/TCP | Internal to host | Conversion host | Local NBDKit server used to stream disk data during conversion (no firewall rule required).
|===

=== On the VMware side

==== Disk Consolidation

Ensure that all virtual machine disks are consolidated before migration.

Virtual machines with unconsolidated disks may fail during migration or result in data inconsistencies.
Disk consolidation merges redundant delta disks created by snapshots back into the base disk.

To check if disks need consolidation:

* In vSphere Client, check the VM summary tab for "Consolidation needed" warnings
* Via PowerCLI: `Get-VM | Where-Object {$_.ExtensionData.Runtime.ConsolidationNeeded}`

To consolidate disks:

* In vSphere Client: Right-click the VM → Snapshots → Consolidate
* Ensure the operation completes successfully before proceeding with migration

==== Snapshot Hierarchy Depth

Verify that the snapshot hierarchy.

* In vSphere Client: Right-click the VM → Snapshots → Manage Snapshots
* Review the snapshot tree depth
* If the hierarchy is too deep, consider consolidating or removing unnecessary snapshots before migration

==== VMware User Access Control Lists (ACLs)

To avoid using the Administrator role and in order to be able to connect, parse the vCenter datastore, manipulate the snapshots and migrate virtual machines, OS-Migrate needs the following ACLs for the vCenter user:

[cols="1,2,3"]
|===
|Category |Privilege Group |Privileges

|**Datastore**
|—
|Browse datastore

|**Virtual Machine**
|Guest operations
|All

|
|Provisioning
|Allow disk access +
Allow file access +
Allow read-only disk access +
Allow virtual machine download

|
|Service configuration
|Allow notifications +
Allow polling of global event notifications +
Read service configuration

|
|Snapshot management
|Create snapshot +
Remove snapshot +
Rename snapshot +
Revert to snapshot
|===

To verify permissions:

* In vCenter: Administration → Access Control → Roles
* Review the assigned role for the migration user
* Ensure all required privileges are granted

==== Change Block Tracking (CBT)

Change Block Tracking is recommended for near-zero downtime migrations with large disks.

[NOTE]
CBT allows incremental data transfer by tracking changed disk blocks between snapshots,
significantly reducing downtime during the final synchronization phase.

==== Enabling CBT

CBT must be enabled on the virtual machine before migration.

**Prerequisites for enabling CBT:**

* VMware Tools must be installed on the VM (see below)
* VM must be powered off or a snapshot must be taken for changes to take effect
* vSphere 4.0 or later

**To enable CBT:**

. Power off the virtual machine (or plan for a snapshot)
. Edit VM settings and add/modify the following advanced parameters:
** `ctkEnabled = TRUE`
** `scsi0:0.ctkEnabled = TRUE` (for each disk, e.g., scsi0:1, scsi0:2, etc.)

. Power on the virtual machine

**Verify CBT is enabled:**

Via vSphere Web Client:
[source,bash]
----
# Check VM configuration for changeTrackingEnabled parameter
----

[NOTE]
If CBT is not enabled, OS Migrate will perform a full disk copy in a single pass,
which may result in longer downtime for VMs with large disks.

==== VMware Tools Installation

VMware Tools installation status should be verified before migration.

**Importance:**

* **For standard migrations:** Recommended but not mandatory
** Improves guest OS detection and metadata gathering
** Enables graceful shutdown capabilities
** Provides better VM customization options post-migration

* **For CBT-based migrations:** Mandatory
** CBT functionality requires VMware Tools to be installed
** Without VMware Tools, CBT cannot be enabled or used

**To check VMware Tools status:**

Via vSphere Client:

* Select the VM and check the Summary tab
* Look for "VMware Tools" status: should show "Running" or "OK"
* Status of "Not installed" or "Not running" indicates action is needed

Via PowerCLI:
[source,powershell]
----
Get-VM <vm-name> | Select Name, @{N='Tools Status';E={$_.ExtensionData.Guest.ToolsStatus}}
----

**To install VMware Tools:**

. In vSphere Client: Right-click the VM → Guest OS → Install VMware Tools
. Follow the guest OS-specific installation process
. Verify the installation completes successfully

[WARNING]
====
Attempting a CBT-based migration without VMware Tools installed will fail.
Ensure VMware Tools are installed and running before enabling CBT.
====

== Workflow

There is different ways to run the migration from VMWare to OpenStack.

* The default is by using nbdkit server with a conversion host (an Openstack instance hosted in the destination cloud).
This way allow the user to use the CBT option and approach a zero downtime. It can also run the migration in one time cycle.
* The second one by using virt-v2v binding with a conversion host. Here you can use a conversion
host (Openstack instance) already deployed or you can let OS-Migrate deployed a conversion host
for you.
* A third way is available where you can skip the conversion host and perform the migration on a Linux machine, the volume
migrated and converted will be upload a Glance image or can be use later as a Cinder volume. This way is not recommended if
you have big disk or a huge amount of VMs to migrate: the performance are really slower than with the other ways.

All of these are configurable with Ansible boolean variables.

== Features and supported OS

=== Features

The following features are availables:

* Discovery mode
* Network mapping
* Port creation and mac addresses mapping
* Openstack flavor mapping and creation
* Migration with nbdkit server with change block tracking feature (CBT)
* Migration with virt-v2v
* Upload migrate volume via Glance
* Multi disks migration
* Multi nics
* Parallel migration on a same conversion host
* Ansible Automation Platform (AAP)


=== Supported OS

The VMware Migration Toolkit uses virt-v2v for conversion. For a list of
supported guest operating systems for virt-v2v, see the Red Hat Knowledgebase article:
https://access.redhat.com/articles/1351473[ Converting virtual machines from other hypervisors to KVM with virt-v2v in RHEL 7, RHEL 8, RHEL 9, and RHEL 10].

RHOSO uses Kernel-based Virtual Machine (KVM) for hypervisors. For a list of certified
guest operating systems for KVM, see the Red Hat Knowledgebase article:
https://access.redhat.com/articles/certified-hypervisors[Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization, Red Hat OpenShift Virtualization and Red Hat Enterprise Linux with KVM].


=== Nbdkit migration example

image::https://raw.githubusercontent.com/os-migrate/documentation/main/images/osm-migration-nbdkit-vmware-workflow-with-osm.drawio.svg[Ndbkit]


=== Nbdkit migration example with the Change Block Tracking

==== Step 1: The data are copied and the change ID from the VMware disk are set to the Cinder volume as metadata

[NOTE]
The conversion cannot be made at this moment, and the OS instance is not created.
This functionality can be used for large disks with a lot of data to transfer. It helps avoid a prolonged service interruption.

image::https://raw.githubusercontent.com/os-migrate/documentation/main/images/osm-migration-nbdkit-vmware-workflow-with-osm_cbt_step1.svg[CBT Step 1]

==== Step 2: OSM compare the source (VMware disk) and the destination (Openstack Volume) change ID

[NOTE]
If the change IDs are not equal, the changed blocks between the source and destination are synced.
Then, the conversion to libvirt/KVM is triggered, and the OpenStack instance is created.
This allows for minimal downtime for the VMs.

image::https://raw.githubusercontent.com/os-migrate/documentation/main/images/osm-migration-nbdkit-vmware-workflow-with-osm_cbt_step2.svg[CBT Step 2]

=== Migration demo from an AEE

The content of the Ansible Execution Environment could be find here:

https://github.com/os-migrate/aap/blob/main/aae-container-file

And the live demo here:

https://www.youtube.com/watch?v=XnEQ8WVGW64[Migration from VMware to OpenStack]

=== Running migration

==== Conversion host

You can use os_migrate.os_migration collection to deploy a conversion, but you can
easily create your conversion host manually.

A conversion host is basically an OpenStack instance.

[NOTE]
Important: If you want to take benefit of the current supported OS, it's highly
recommended to use a *CentOS-10* release or *RHEL-9.5* and superior. If you want
to use other Linux distribution, make sure the virtio-win package is equal or higher than 1.40 version.

[source,bash]
----
curl -O -k https://cloud.centos.org/centos/10-stream/x86_64/images/CentOS-Stream-GenericCloud-10-20250217.0.x86_64.qcow2

# Create OpenStack image:
openstack image create --disk-format qcow2 --file CentOS-Stream-GenericCloud-10-20250217.0.x86_64.qcow2 CentOS-Stream-GenericCloud-10-20250217.0.x86_64.qcow2

# Create flavor, security group and network if needed
openstack server create --flavor x.medium --image 14b1a895-5003-4396-888e-1fa55cd4adf8  \
  --key-name default --network private   vmware-conv-host
openstack server add floating ip vmware-conv-host 192.168.18.205
----

==== VMware VDDK setup

===== Download and extract the VMware VDDK

. In a browser, navigate to the VMware VDDK download page.
. Select version 8.0.1 and download the archive.
. Save the archive in a temporary directory and extract it:

[source,bash]
----
tar -xzf VMware-vix-disklib-<version>.x86_64.tar.gz
----

You can then specify the library path via:

[source,yaml]
----
conversion_host_vmware_vix_disklib: /usr/lib/vmware-vix-disklib
----

If you want to skip the `conversion_host` role entirely, specify the library path on the migrator instead:

[source,yaml]
----
import_workloads_libdir: /usr/lib/vmware-vix-disklib
----

==== Inventory, Variables files and Ansible command:

**inventory.yml**

[source,yaml]
----
migrator:
  hosts:
    localhost:
      ansible_connection: local
      ansible_python_interpreter: "{{ ansible_playbook_python }}"
conversion_host:
  hosts:
    192.168.18.205:
      ansible_ssh_user: cloud-user
      ansible_ssh_private_key_file: key
----

**myvars.yml:**

[source,yaml]
----
# if you run the migration from an Ansible Execution Environment (AEE)
# set this to true:
runner_from_aee: true

# osm working directory:
os_migrate_vmw_data_dir: /opt/os-migrate
copy_openstack_credentials_to_conv_host: false

# Re-use an already deployed conversion host:
already_deploy_conversion_host: true

# If no mapped network then set the openstack network:
openstack_private_network: 81cc01d2-5e47-4fad-b387-32686ec71fa4

# Security groups for the instance:
security_groups: ab7e2b1a-b9d3-4d31-9d2a-bab63f823243
use_existing_flavor: true
# key pair name, could be left blank
ssh_key_name: default
# network settings for openstack:
os_migrate_create_network_port: true
copy_metadata_to_conv_host: true
used_mapped_networks: false

vms_list:
  - rhel-9.4-1
----

**secrets.yml:**

[source,yaml]
----
# VMware parameters:
esxi_hostname: 10.0.0.7
vcenter_hostname: 10.0.0.7
vcenter_username: root
vcenter_password: root
vcenter_datacenter: Datacenter

os_cloud_environ: psi-rhos-upgrades-ci
dst_cloud:
  auth:
    auth_url: https://keystone-public-openstack.apps.ocp-4-16.standalone
    username: admin
    project_id: xyz
    project_name: admin
    user_domain_name: Default
    password: openstack
  region_name: regionOne
  interface: public
  insecure: true
  identity_api_version: 3
----

**Ansible command:**

[source,bash]
----
ansible-playbook -i inventory.yml os_migrate.vmware_migration_kit.migration -e @secrets.yml -e @myvars.yml
----

== Usage

You can find a "how to" here, to start from sratch with a container:
https://gist.github.com/matbu/003c300fd99ebfbf383729c249e9956f

Clone repository or install from ansible galaxy

[source,bash]
----
git clone https://github.com/os-migrate/vmware-migration-kit
ansible-galaxy collection install os_migrate.vmware_migration_kit
----

=== Nbdkit (default)

Edit vars.yaml file and add our own setting:

[source,yaml]
----
esxi_hostname: ********
vcenter_hostname: *******
vcenter_username: root
vcenter_password: *****
vcenter_datacenter: Datacenter
----

If you already have a conversion host, or if you want to re-used a previously deployed one:

[source,yaml]
----
already_deploy_conversion_host: true
----

Then specify the Openstack credentials:

[source,yaml]
----
# OpenStack destination cloud auth parameters:
dst_cloud:
  auth:
    auth_url: https://openstack.dst.cloud:13000/v3
    username: tenant
    project_id: xyz
    project_name: migration
    user_domain_name: osm.com
    password: password
  region_name: regionOne
  interface: public
  identity_api_version: 3

# OpenStack migration parameters:
# Use mapped networks or not:
used_mapped_networks: true
network_map:
  VM Network: private

# If no mapped network then set the openstack network:
openstack_private_network: 81cc01d2-5e47-4fad-b387-32686ec71fa4

# Security groups for the instance:
security_groups: 4f077e64-bdf6-4d2a-9f2c-c5588f4948ce
use_existing_flavor: true

os_migrate_create_network_port: false

# OS-migrate parameters:
# osm working directory:
os_migrate_vmw_data_dir: /opt/os-migrate

# Set this to true if the Openstack "dst_cloud" is a clouds.yaml file
# other, if the dest_cloud is a dict of authentication parameters, set
# this to false:
copy_openstack_credentials_to_conv_host: false

# Teardown
# Set to true if you want osm to delete everything on the destination cloud.
os_migrate_tear_down: true

# VMs list
vms_list:
  - rhel-1
  - rhel-2
----

=== OpenStack Flavor

When using VMware as a source, there are several ways to handle the flavor for the resulting OpenStack instance. VMware has no native flavor concept, so OS-Migrate supports:

. Find the closest matching flavor
** Enable: `use_existing_flavor: true`

** If no flavor matches, OS‑Migrate will create one automatically.

. Create a new flavor for each VM
** The created flavor name follows:
   `osm-vmware-<vm_name>-<random_id>` (example: `osm-vmware-myvm-9999`).

. Provide a specific flavor UUID
** Force usage of an existing flavor: `flavor_uuid: <your_flavor_uuid>`
** Useful to define custom properties for host aggregation or targeted placement.

=== Host Aggregates

Host aggregates are a mechanism for partitioning hosts in an OpenStack cloud, or a region of an OpenStack cloud, based on arbitrary characteristics.
More information on how to configure the host aggregates in OpenStack can be found here: https://docs.openstack.org/nova/latest/admin/aggregates.html

Once you have set up the host aggregates you can now create the flavor with the `openstack flavor create` command.
Once the flavor is created, specify one or more key-value pairs that match the key-value pairs on the host aggregates with scope `aggregate_instance_extra_specs`.

When you have created in your OpenStack destination cloud the flavor with the correct property, you can now specify to os-migrate the variable:

`flavor_uuid: <your_flavor_uuid>`

=== Encryption

If you use encryption with our VMware virtual machine (VMware encryption or encrypted
file system), you can provide the key or the passphrase via the 'v2v_extra_opt'

[source,yaml]
----
v2v_extra_opt: "--key all:key:mypassphrase"
# or
v2v_extra_opt: "--key all:file:mykeyfile"
----

The `v2v_extra_opt` is a wrapper on https://libguestfs.org/virt-v2v.1.html parameters.
You can append options via this variable.

=== First boot script

You can inject during the migration, at the conversion step, a script or a command
which will be executed a the first boot of the new OpenStack disk life.

[source,yaml]
----
v2v_first_boot_script: "/path/to/firstboot.sh"
----

The first boot script should already present on the conversion host.

=== Virt‑v2v Wrapper (not recommended)

Provide the following additional information when using virt‑v2v:

[source,yaml]
----
# virt‑v2v parameters
vddk_thumbprint: XX:XX:XX
vddk_libdir: /usr/lib/vmware-vix-disklib
----

Generate the thumbprint of your VMware source cloud with:

[source,bash]
----
openssl s_client -connect ESXI_SERVER_NAME:443 </dev/null | \
  openssl x509 -in /dev/stdin -fingerprint -sha1 -noout
----

=== Running migration from local shared NFS
OS-Migrate can migrate directly from a local shared directory mounted on the
conversion host. If the VMware virtual machines are located on an NFS datastore
that is accessible to the conversion host, you can mount the NFS storage on the
conversion host and provide the path to the NFS mount point.

OS-Migrate will then directly consume the disks of the virtual machines located
on the NFS mount point. Configure the Ansible variable to specify your mount
point as follows:

[source,yaml]
----
import_workloads_local_disk_path: "/srv/nfs"
----

[NOTE]
In this mode, only cold migration is supported.

=== Ansible configuration

Create an invenvoty file, and replace the conv_host_ip by the ip address of your
conversion host:

[source,yaml]
----
migrator:
  hosts:
    localhost:
      ansible_connection: local
      ansible_python_interpreter: "{{ ansible_playbook_python }}"
conversion_host:
  hosts:
    conv_host_ip:
      ansible_ssh_user: cloud-user
      ansible_ssh_private_key_file: /home/stack/.ssh/conv-host
----

Then run the migration with:

[source,bash]
----
ansible-playbook -i localhost_inventory.yml os_migrate.vmware_migration_kit.migration -e @vars.yaml
----

=== Running Migration outside of Ansible

You can also run migration outside of Ansible because the Ansible module are written in Golang.
The binaries are located in the plugins directory.

From your conversion host (or an Openstack instance inside the destination cloud) you need to export
Openstack variables:

[source,bash]
----
 export OS_AUTH_URL=https://keystone-public-openstack.apps.ocp-4-16.standalone
 export OS_PROJECT_NAME=admin
 export OS_PASSWORD=admin
 export OS_USERNAME=admin
 export OS_DOMAIN_NAME=Default
 export OS_PROJECT_ID=xyz
----

Then create the argument json file, for example:

[source,json]
----
cat <<EOF > args.json
{
		"user": "root",
		"password": "root",
		"server": "10.0.0.7",
		"vmname": "rhel-9.4-3",
		"cbtsync": false,
		"dst_cloud": {
			"auth": {
				"auth_url": "https://keystone-public-openstack.apps.ocp-4-16.standalone",
				"username": "admin",
				"project_id": "xyz",
				"project_name": "admin",
				"user_domain_name": "Default",
				"password": "admin"
			},
			"region_name": "regionOne",
			"interface": "public",
			"identity_api_version": 3
		}
}
EOF
----

Then execute the `migrate` binary:

[source,bash]
----
pushd vmware-migration-kit/vmware_migration_kit
./plugins/modules/migrate/migrate
----

You can see the logs into:

[source,bash]
----
tail -f /tmp/osm-nbdkit.log
----

== Troubleshooting

=== Connectivity from Conversion Host to VMware

Ensure network and name resolution are properly configured before running migrations.

* Port 902 must be reachable from the conversion host:
[source,bash]
----
curl -v telnet://<vcenter_ip>:902
# or
nc -zv <vcenter_ip> 902
----
The connection should succeed.

* vCenter FQDN resolution
  Ensure the vCenter hostname resolves from the conversion host. If necessary, update `/etc/hosts`:
[source,bash]
----
echo "<vcenter_ip> vcenter.domain.local" | sudo tee -a /etc/hosts
----

=== OpenStack Metadata service

If the metadata service is not reachable you may see errors like:

```
Failed to fetch metadata: Get "http://169.254.169.254/openstack/latest/meta_data.json": dial tcp 169.254.169.254:80: connect: no route to host
```

As a workaround you can set a manual instance UUID in the import playbook:

[source,yaml]
----
import_workloads_instance_uuid: <uuid>
----

=== Enable Debugging Flags During Migration

Increase verbosity and capture detailed debug information by setting:

[source,yaml]
----
import_workloads_debug: true
----

OS-Migrate creates a unique log file per migration on the conversion host under `/tmp`, and in case of failure pulls it back to the OS-Migrate work directory (default `/opt/os-migrate`) under a folder named after the VM. The log naming format is:
`osm-nbdkit-<vm-name>-<random-id>.log`.

[source,bash]
----
tail -f /tmp/osm-nbdkit-<vm-name>-<random-id>.log
----

=== NBDKit errors

If you encounter:

```
nbdkit: error: server has no export named '': No such file or directory
```

Common causes:

1. Port 902 not open between conversion host and vCenter.
2. vCenter FQDN not resolvable.
3. Malformed `nbdkit` command (invalid characters or parameters).

=== Manual debug procedure

You can replay the commands manually for troubleshooting.

==== Step 1 – Run `nbdkit` manually

Run the command shown in the logs with `--verbose` and wrap the VMDK path in double quotes:

[source,bash]
----
nbdkit --verbose vddk ".../guest-00001.vmdk"
----

If the migration snapshot has been deleted, remove the snapshot option and use the base disk instead.

==== Step 2 – Run `nbdcopy` in another shell

Run the `nbdcopy` command as shown in the logs and observe `nbdkit` output. You should see:
`vddk: config_complete.`

==== Step 3 – Analyze authentication and paths

At this point, authentication was already verified by the migration process. The VMDK path is returned by the VMware API, typically:
`[Datastore 1] path/to/the/guest-00001.vmdk`.
